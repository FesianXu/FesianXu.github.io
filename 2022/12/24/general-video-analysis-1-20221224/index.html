<!doctypehtml><html class="theme-next muse use-motion" lang=zh-CN><meta charset=UTF-8><meta content=IE=edge http-equiv=X-UA-Compatible><meta content=width=device-width,initial-scale=1,maximum-scale=1 name=viewport><meta content=#222 name=theme-color><meta content=no-transform http-equiv=Cache-Control><meta content=no-siteapp http-equiv=Cache-Control><link href=/lib/fancybox/source/jquery.fancybox.css?v=2.1.5 rel=stylesheet><link href=/lib/font-awesome/css/font-awesome.min.css?v=4.6.2 rel=stylesheet><link href=/css/main.css?v=5.1.4 rel=stylesheet><link href=/images/apple-touch-icon-next.png?v=5.1.4 rel=apple-touch-icon sizes=180x180><link href=/images/favicon-32x32-next.png?v=5.1.4 rel=icon sizes=32x32 type=image/png><link href=/images/favicon-16x16-next.png?v=5.1.4 rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg?v=5.1.4 rel=mask-icon><meta content=多模态模型,多模态融合,视频分析,视频理解,语义标签, name=keywords><meta content=在前文《万字长文漫谈视频理解》[1]中，笔者曾经对视频理解中常用的一些技术进行了简单介绍，然而限于篇幅，意犹未尽。在实习工作中，笔者进一步接触了更多视频分析在视频搜索中的一些应用，深感之前对视频分析在业界中应用的理解过于狭隘。本文作为笔者对前文的一个补充，进一步讨论一下视频分析以及其在搜索推荐系统中的一些应用。本文是该系列的第一篇，主要介绍了通用的视频图片语义以及讨论了多模态模型的必要性。 name=description><meta content=article property=og:type><meta content=视频分析与多模态融合之一，为什么需要多模态融合 property=og:title><meta content=https://fesianxu.github.io/2022/12/24/general-video-analysis-1-20221224/index.html property=og:url><meta content=机器学习杂货铺总店 property=og:site_name><meta content=在前文《万字长文漫谈视频理解》[1]中，笔者曾经对视频理解中常用的一些技术进行了简单介绍，然而限于篇幅，意犹未尽。在实习工作中，笔者进一步接触了更多视频分析在视频搜索中的一些应用，深感之前对视频分析在业界中应用的理解过于狭隘。本文作为笔者对前文的一个补充，进一步讨论一下视频分析以及其在搜索推荐系统中的一些应用。本文是该系列的第一篇，主要介绍了通用的视频图片语义以及讨论了多模态模型的必要性。 property=og:description><meta content=zh_CN property=og:locale><meta content=https://fesianxu.github.io/2022/12/24/general-video-analysis-1-20221224/qrcode.png property=og:image><meta content=https://fesianxu.github.io/2022/12/24/general-video-analysis-1-20221224/imgs/video_no_human.png property=og:image><meta content=https://fesianxu.github.io/2022/12/24/general-video-analysis-1-20221224/imgs/image_language_pair.png property=og:image><meta content=https://fesianxu.github.io/2022/12/24/general-video-analysis-1-20221224/imgs/label_semantics.png property=og:image><meta content=https://fesianxu.github.io/2022/12/24/general-video-analysis-1-20221224/imgs/howto_combine.png property=og:image><meta content=https://fesianxu.github.io/2022/12/24/general-video-analysis-1-20221224/imgs/Howto100m_categories.png property=og:image><meta content=https://fesianxu.github.io/2022/12/24/general-video-analysis-1-20221224/imgs/semantic_classification_label.png property=og:image><meta content=https://fesianxu.github.io/2022/12/24/general-video-analysis-1-20221224/imgs/concept_label.png property=og:image><meta content=2022-12-24T03:04:54.000Z property=article:published_time><meta content=2022-12-24T03:22:19.525Z property=article:modified_time><meta content=FesianXu property=article:author><meta content=多模态模型 property=article:tag><meta content=多模态融合 property=article:tag><meta content=视频分析 property=article:tag><meta content=视频理解 property=article:tag><meta content=语义标签 property=article:tag><meta content=summary name=twitter:card><meta content=https://fesianxu.github.io/2022/12/24/general-video-analysis-1-20221224/qrcode.png name=twitter:image><script id=hexo.configurations>var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };</script><link href=https://FesianXu.github.io/2022/12/24/general-video-analysis-1-20221224/ rel=canonical><title>视频分析与多模态融合之一，为什么需要多模态融合 | 机器学习杂货铺总店</title><meta content="Hexo 7.3.0" name=generator><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}</style><body itemscope itemtype=http://schema.org/WebPage lang=zh-CN><div class="container sidebar-position-left page-post-detail"><div class=headband></div><header class=header id=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-wrapper><div class=site-meta><div class=custom-logo-site-title><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <span class=site-title>机器学习杂货铺总店</span> <span class=logo-line-after><i></i></span> </a></div><p class=site-subtitle></div><div class=site-nav-toggle><button><span class=btn-bar></span> <span class=btn-bar></span> <span class=btn-bar></span></button></div></div><nav class=site-nav><ul class=menu id=menu><li class="menu-item menu-item-home"><a href=/ rel=section> <i class="menu-item-icon fa fa-fw fa-home"></i> <br> Home </a><li class="menu-item menu-item-about"><a href=/about/ rel=section> <i class="menu-item-icon fa fa-fw fa-user"></i> <br> About </a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section> <i class="menu-item-icon fa fa-fw fa-tags"></i> <br> Tags </a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section> <i class="menu-item-icon fa fa-fw fa-th"></i> <br> Categories </a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section> <i class="menu-item-icon fa fa-fw fa-archive"></i> <br> Archives </a><li class="menu-item menu-item-search"><a class=popup-trigger href=javascript:;> <i class="menu-item-icon fa fa-search fa-fw"></i> <br> Search </a></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon> <i class="fa fa-search"></i> </span><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span><div class=local-search-input-wrapper><input autocomplete=off id=local-search-input placeholder=Searching... spellcheck=false></div></div><div id=local-search-result></div></div></div></nav></div></header><main class=main id=main><div class=main-inner><div class=content-wrap><div class=content id=content><div class=posts-expand id=posts><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://FesianXu.github.io/2022/12/24/general-video-analysis-1-20221224/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/%5Bobject%20Object%5D itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=机器学习杂货铺总店 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>视频分析与多模态融合之一，为什么需要多模态融合</h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>Posted on</span> <time itemprop="dateCreated datePublished" title="Post created" datetime=2022-12-24T11:04:54+08:00> 2022-12-24 </time> </span><span class=post-category> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-folder-o"></i> </span> <span class=post-meta-item-text>In</span> <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B/ itemprop=url rel=index> <span itemprop=name>多模态模型</span> </a> </span> </span><span class=post-meta-divider>|</span><span class=page-pv><i class="fa fa-file-o"></i> <span class=busuanzi-value id=busuanzi_value_page_pv></span> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-file-word-o"></i> </span><span class=post-meta-item-text>Words count in article:</span><span title="Words count in article"> 4.3k 字 </span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span class=post-meta-item-text>Reading time ≈</span><span title="Reading time"> 15 分钟 </span></div></div></header><div class=post-body itemprop=articleBody><p>在前文《万字长文漫谈视频理解》[1]中，笔者曾经对视频理解中常用的一些技术进行了简单介绍，然而限于篇幅，意犹未尽。在实习工作中，笔者进一步接触了更多视频分析在视频搜索中的一些应用，深感之前对视频分析在业界中应用的理解过于狭隘。本文作为笔者对前文的一个补充，进一步讨论一下视频分析以及其在搜索推荐系统中的一些应用。本文是该系列的第一篇，主要介绍了通用的视频图片语义以及讨论了多模态模型的必要性。 <span id=more></span><div align=right>FesianXu 20210130 at Baidu search team</div><h1 id=前言>前言</h1><p>在前文《万字长文漫谈视频理解》[1]中，笔者曾经对视频理解中常用的一些技术进行了简单介绍，然而限于篇幅，意犹未尽。在实习工作中，笔者进一步接触了更多视频分析在视频搜索中的一些应用，深感之前对视频分析在业界中应用的理解过于狭隘。本文作为笔者对前文的一个补充，进一步讨论一下视频分析以及其在搜索推荐系统中的一些应用。本文是该系列的第一篇，主要介绍了通用的视频图片语义以及讨论了多模态模型的必要性。<p><strong>如有谬误请联系指出，本文遵守<a href=http://creativecommons.org/licenses/by-sa/4.0/ rel=noopener target=_blank>CC 4.0 BY-SA</a>版权协议，转载请联系作者并注明出处，谢谢</strong>。<p><span class="math inline"><mjx-container class=MathJax jax=SVG><svg style="vertical-align: -0.075ex;" viewbox="0 -683 833 716" focusable=false height=1.62ex role=img width=1.885ex xmlns=http://www.w3.org/2000/svg><g fill=currentColor stroke=currentColor stroke-width=0 transform=scale(1,-1)><g data-mml-node=math><g data-mml-node=mi><path d="M46 676Q46 679 51 683H781Q786 679 786 676Q786 674 617 326T444 -26Q439 -33 416 -33T388 -26Q385 -22 216 326T46 676ZM697 596Q697 597 445 597T193 596Q195 591 319 336T445 80L697 596Z" data-c=2207 /></g></g></g></svg></mjx-container></span> 联系方式：<p><strong>e-mail</strong>: FesianXu@gmail.com<p><strong>github</strong>: https://github.com/FesianXu<p><strong>知乎专栏</strong>: <a href=https://zhuanlan.zhihu.com/c_1265262560611299328 rel=noopener target=_blank>计算机视觉/计算机图形理论与应用</a><p><strong>微信公众号</strong>：机器学习杂货铺3号店<p><img src=/2022/12/24/general-video-analysis-1-20221224/qrcode.png><hr><p>注：本文同样是属于漫谈性质的博客，因此行文较为随意，可能逻辑脉路不是特别清晰，各处充盈笔者的随笔漫想，如有谬误，请各位读者谅解并指出讨论。阅读本文之前，可以先阅读笔者之前的一篇博文[1]，以确保叙事的承前启后以及完整性。<h1 id=视频分析不仅有动作识别>视频分析不仅有动作识别</h1><p>之前笔者写过一篇长篇博客[1]，主要介绍了一些视频理解的技术，其中偏重于基于动作识别技术的视频分析，包括数据集和模型设计方法都是偏向于动作识别的。其中学术界中，动作分析数据集最常用的包括：<code>HMDB-51</code>，<code>ucf-101</code>, <code>sports-1M</code>，<code>Kinectics</code>等，这些数据集的标签基本上都是一些动作类型，大部分是为了动作识别任务而特别筛选过的YouTube视频。视频动作识别技术在安防领域有着广泛地使用场景，特别地，安防领域对于<strong>多视角</strong>视频动作识别技术有着更为急切的需求，因为在这种场景中，摄像头需要部署在各种可能的地方，因此摄像机姿态各异，需要利用多视角的方法挖掘不同视角下的共同表征，以减少对视角不同场景中重新收集数据的需求。同时，多视角动作识别也会和行人重识别（ReID）技术有着一些交叠，不过那是后话了。<p>然而，在互联网场景中，以视频搜索/推荐系统为例子，我们需要面对的是用户上传的各种各样的视频，这些视频中语义复杂，不单单是人体动作识别能够简单解决的，甚至很多用户视频并没有包括人的出场。笔者因为在研究生阶段研究的是基于人体的视频分析，因此在前文[1]中没能对互联网中用户视频类型进行准确判断，进而有以下判断<blockquote><p>视频动作分析可以视为视频理解的核心</blockquote><p>这个论述不能说完全错误，以人为主要出场的视频的视频理解的<strong>核心思路之一</strong>的确是动作分析，但是，首先考虑到线上很多视频是用户拍摄周围的景色，或者是动漫片段，或者是其他类型的视频，如Fig 1.1所示，有很多视频并没有人的出场，自然动作分析也就失效了。其次，视频语义和视频中的动作语义并不是完全对齐的，举个例子，一个视频中两个人在碰杯喝酒，我们通过动作识别模型只能知道这两个人在碰杯喝酒，仅此而已。但是其实视频中的两个人是为了庆祝某件重大的事情才碰杯喝酒的，这个“重大的事情”才是整个视频强调的语义，在目前的搜索系统中大多只能通过Query-Title匹配/语义分析的方法才能对此进行召回和排序，如果搜索模型把关注点集中到了视频的动作语义，就难以对该视频进行召回和排序了。<p><img src=/2022/12/24/general-video-analysis-1-20221224/imgs/video_no_human.png><div align=center><b> Fig 1.1 有很多视频并没有人的出场，此时基于人体动作识别的方法不能准确捕捉视频语义信息。 </b></div><p>总结来说，视频理解是一个非常宏大的话题，笔者在[1]中只是对视频动作识别进行了简单的介绍，应用场景比较受限于智能监控分析场景，远远还达不到能在互联网线上系统使用的程度。通用的视频理解，面临着数据集缺少，视频语义复杂多变，视频多模态语义融合，非线性流的视频理解等诸多问题，笔者很难在这些领域都有全面的了解，笔者尽量在本文介绍一些自己知道的工作，希望对读者有所帮助。<h1 id=数据集的补充介绍>数据集的补充介绍</h1><p>在[1]中我们介绍过一些常用的动作识别数据集，这些数据集在通用视频分析中并不太够用，目前存在有一些多模态数据集，也存在有一些现实场景中通用的视频数据集，还有一类型称之为<code>HowTo</code>类型视频的数据集。<h2 id=多模态数据集>多模态数据集</h2><p>在搜索场景中，用户给出检索词Query，需要返回合适的视频给用户，在这个过程中，涉及到了模型对Query与视频内容，视频标题等诸多元素之间相关性的度量问题。因此这并不是一个简单的对视频进行特征提取的过程，而且涉及到了文本-视觉多模态之间的特征融合，交互过程。据笔者了解，目前存在有若干多模态相关的数据集，很多都用于Image caption或者video caption，Video QA，Image QA等任务，通过迁移也可以应用在Query与视频之间的特征交互。以下列举几种常用于Visual+Language任务中用于预训练的数据集：<p><strong>COCO Caption</strong> [4]： 从COCO数据集中进行采样，然后让人工进行一句话描述图片内容的样本对 <code>&LTimage, text-description></code>，可用于V+L任务的预训练。<p><strong>Visual Genome Dense Captions</strong> [5]: 类似于COCO Caption，从Visual Genome数据中采集而成。<p><strong>Conceptual Captions</strong> [6]: 类似于COCO Caption<p><strong>SBU Caption</strong> [7]: 类似于COCO Caption<p>这些数据如Fig 2.1所示，一般是图文对的形式出现的，一幅图片搭配有起码一句人工描述的文本，有些数据集可能会有<code>alt-text</code>等，如Conceptual Caption数据集。<p><img src=/2022/12/24/general-video-analysis-1-20221224/imgs/image_language_pair.png><div align=center><b> Fig 2.1 图片-文本对的形式的多模态数据集，常用于进行预训练。 </b></div><p>这里谈到的四种数据集的数据集大小以及一些小细节在[10]中的<code>in-domain和out-of-domain</code>一节有过介绍，这里不再累述。<h2 id=youtube数据集>YouTube数据集</h2><p>YouTube有着海量的视频资源，有很多数据集也是从YouTube中进行采样得到的，其中包括Kinetics系列数据集，YouTube 8M数据集等，其中YouTube 8M数据集具有6.1M的视频量，视频时长总计大约为350K小时，一共有3862个标签，平均每个视频有3个标签，其标签的语义包括了诸多日常场景，如Fig2.2所示，可见其实一个明显的长尾分布。<p><img src=/2022/12/24/general-video-analysis-1-20221224/imgs/label_semantics.png><div align=center><b> Fig 2.2 YouTube 8M数据集中的标签的语义范围。 </b></div><p>YouTube 8M因为数据量庞大，没有提供每个视频的原始帧，而是提供了用CNN模型处理过后的特征，该特征是用CNN模型对数据集中每个视频的每帧进行特征提取后得到的。基于这种帧特征化后的数据集，之前谈到的一些光流方法，3D卷积方法将无法使用。然而在线上的实践中，这种方法还是最为常见的。<p>除了YouTube 8M之外，还有MSR-VTT [11]也是采集于YouTube的通用视频数据集。<h2 id=instructional数据集>Instructional数据集</h2><p>Instructional视频，是一类视频的总称，意在教导人们如何完成某件事情，因此也称之为HowTo视频，如Fig 2.3所示，这类型视频的特点就是会出现一个人，以语音解说伴随着动作指导观众完成某个事情。这一类视频在网络视频中具有一定的比重，因此对其的文本语义-视觉信息的语义对齐是很值得研究的一个问题。目前公开用于预训练或者模型验证的数据集有以下几个。<p><img src=/2022/12/24/general-video-analysis-1-20221224/imgs/howto_combine.png><div align=center><b> Fig 2.3 HowTo视频的示例。 </b></div><p><strong>HowTo100M</strong> [11]：该数据集通过在WikiHow [13]中挑选了23,611个howto任务，然后依次为检索词query在YouTube上进行搜索，然后将前200个结果进行筛选，得到了最后的数据集，一共有136.6M个视频。因为这类型的视频普遍存在语音解说，作者用ASR（Automatic Speech Recognition）去提取每个视频每一帧的解说语音（如果该视频本身由作者上传有字幕，则使用原生的字幕信息），将其转换成文本形式的叙述（narrations），也即是此时存在<code><文本叙述，视频帧></code>的样本对，通过这种手段，作者生成了大规模的带噪声的文本-视频样本对用于多模态任务的预训练。将howto任务分为12个大类，如Fig 2.4所示，我们发现howto视频也是呈现一个典型的长尾分布。<p><img src=/2022/12/24/general-video-analysis-1-20221224/imgs/Howto100m_categories.png><div align=center><b> Fig 2.4 从WikiHow中挑选的12个大类的howto类别。 </b></div><h1 id=多模态语义融合>多模态语义融合</h1><p>什么叫做多模态呢？我们之前已经谈到过了，无非是对于同一个概念，同一个事物通过不同的模态的媒体进行描述，比如用图片，用视频，用语言，用语音对某一个场景进行描述，这就是多模态的一个例子。多模态目前是一个很火的研究方向，我们在上文讨论中发现，目前视频语义复杂，特别是在搜索推荐系统中，可能包含有各种种类的视频，光从动作语义上很难进行描述。如果扩充到其他更广阔的语义，则需要更加精细的标注才能实现。通常而言，动作分类的类别标注就过于粗糙了。<p>考虑到搜索推荐系统中广泛存在的长尾现象，进行事无巨细的样本标注工作显然是不可取的，再回想到我们之前在[15]中谈到的“语义标签”的概念，如Fig 3.1所示，即便有足够的人力进行标注，<strong>如何进行合适的样本标注设计也是一件复杂的问题</strong>。对于一张图（亦或是一个视频），单纯给予一个动作标签不足以描述整个样本的语义，额外对样本中的每个物体的位置，种类进行标注，对每个样本发生的事情进行文本描述，对样本的场景，环境进行描述，这些都是可以采取的进一步的标注方式。<p>就目前而言，据笔者了解，在预训练阶段，为了保证预训练结果能够在下游任务中有效地泛化，不能对预训练的语义进行狭义的约束，比如 <strong>仅</strong> 用动作类别语义进行约束就是一个狭义约束。为了使得标注有着更为通用的语义信息，目前很多采用的是多模态融合的方法。<p><img src=/2022/12/24/general-video-analysis-1-20221224/imgs/semantic_classification_label.png><div align=center><b> Fig 3.1 语义标签可以使得具有相同语义的物体在特征空间更为地接近。如何对样本进行语义标注是一件复杂的事情。 </b></div><p>在多模态融合方法中，以图片为例子，可以考虑用一句话去描述一张图中的元素和内容（此处的描述语句可以是人工标注的，也可以是通过网络的海量资源中自动收集得到的，比如用户对自己上传图片的评论，描述甚至是弹幕等），比如在ERNIE-VIL [16]中采用的预训练数据集Conceptual Captions (CC) dataset [6]，其标注后的样本如Fig 3.2所示。其中的虚线框是笔者添加的，我们注意到左上角的样本，其标注信息是"Trees in a winter snowstorm"，通过这简单一个文本，伴随配对的图片，我们可以知道很多信息：<ol type=1><li>在暴风雪下，天气以白色为主。<li>树的形状和模样，一般是直立在土地上的。<li>暴风雪时候，能见度很低。</ol><p>如果数据集中还有些关于树木的场景的描述文本，比如“Two boys play on the tree”， 那么模型就很有可能联合这些样本，学习到“树（tree）”这个概念，即便没有人类标注的包围盒标签（bounding box label）都可以学习出来，除此之外因为语义标签的通用性，还提供了学习到其他关于暴风雪概念的可能性。通过这种手段，可以一定程度上缓解长尾问题导致的标签标注压力。并且因为文本的嵌入特征具有语义属性，意味着文本标签可以对相近语义的表述（近义词，同义词等等）进行兼容，进一步提高了模型的通用性，这些都是多模态融合模型的特点。<p><img src=/2022/12/24/general-video-analysis-1-20221224/imgs/concept_label.png><div align=center><b> Fig 3.2 用具有更为通用语义的标注方式，进行图片的描述可以大大减少标注压力。 </b></div><p>综上所述，笔者认为，多模态融合模型具有以下优点：<ol type=1><li>对标注的精准性要求更低，可以通过人类直观的看图说话进行标注。<li>可以通过互联网大量收集弱标注的图片描述数据，如图片评论，弹幕，用户的自我描述等。<li>语义更为通用，可作为预训练模型供多种下游任务使用。<li>可以缓解长尾问题，对语义相近的场景更为友好。</ol><p>因此，笔者认为，在当前互联网弱标注数据海量存在的时代，算力大大增强的时代，在跨模态的视频，图片搜索推荐这些应用中，采用多模态融合的方法是势在必行的，是现在和未来的方向。我们后面的系列文章将会对这些方法进行简单的介绍。<h1 id=reference>Reference</h1><p>[1]. https://fesian.blog.csdn.net/article/details/105545703<p>[2]. https://fesian.blog.csdn.net/article/details/108212429<p>[3]. https://fesian.blog.csdn.net/article/details/87901764<p>[4]. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)<p>[5]. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV (2017)<p>[6]. Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In: ACL (2018)<p>[7]. Ordonez, V., Kulkarni, G., Berg, T.L.: Im2text: Describing images using 1 million captioned photographs. In: NeurIPS (2011)<p>[8]. https://github.com/rohit497/Recent-Advances-in-Vision-and-Language-Research<p>[9]. https://github.com/lichengunc/pretrain-vl-data<p>[10]. https://blog.csdn.net/LoseInVain/article/details/103870157<p>[11]. Miech, A., Zhukov, D., Alayrac, J. B., Tapaswi, M., Laptev, I., & Sivic, J. (2019). Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 2630-2640).<p>[12]. Zhou, L., Xu, C., & Corso, J. (2018, April). Towards automatic learning of procedures from web instructional videos. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (Vol. 32, No. 1).<p>[13]. https://www.wikihow.com/<p>[14]. Rohrbach, Anna, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo Larochelle, Aaron Courville, and Bernt Schiele. "Movie description." <em>International Journal of Computer Vision</em> 123, no. 1 (2017): 94-120.<p>[15]. https://blog.csdn.net/LoseInVain/article/details/114958239<p>[16]. Yu, Fei, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. “Ernie-vil: Knowledge enhanced vision-language representations through scene graph.” arXiv preprint arXiv:2006.16934 (2020).</div><div><ul class=post-copyright><li class=post-copyright-author><strong>Post author:</strong> FesianXu<li class=post-copyright-link><strong>Post link:</strong> <a href=https://fesianxu.github.io/2022/12/24/general-video-analysis-1-20221224/ title=视频分析与多模态融合之一，为什么需要多模态融合>https://fesianxu.github.io/2022/12/24/general-video-analysis-1-20221224/</a><li class=post-copyright-license><strong>Copyright Notice: </strong> All articles in this blog are licensed under <a rel="external nofollow" href=https://creativecommons.org/licenses/by-nc-sa/3.0/ target=_blank>CC BY-NC-SA 3.0</a> unless stating additionally.</ul></div><footer class=post-footer><div class=post-tags><a href=/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B/ rel=tag># 多模态模型</a><a href=/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%9E%8D%E5%90%88/ rel=tag># 多模态融合</a><a href=/tags/%E8%A7%86%E9%A2%91%E5%88%86%E6%9E%90/ rel=tag># 视频分析</a><a href=/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/ rel=tag># 视频理解</a><a href=/tags/%E8%AF%AD%E4%B9%89%E6%A0%87%E7%AD%BE/ rel=tag># 语义标签</a></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/2022/12/24/nonlinear-struct-video-20221224/ rel=next title=基于图结构的视频理解——组织视频序列的非线性流> <i class="fa fa-chevron-left"></i> 基于图结构的视频理解——组织视频序列的非线性流 </a></div><span class=post-nav-divider></span><div class="post-nav-prev post-nav-item"><a href=/2022/12/24/for-beginner-IR-system-20221224/ rel=prev title=从零开始的搜索系统学习笔记> 从零开始的搜索系统学习笔记 <i class="fa fa-chevron-right"></i> </a></div></div></footer></div></article><div class=post-spread></div></div></div><div class=comments id=comments><div data-id=city data-uid=MTAyMC82MDMzMS8zNjc5OQ== id=lv-container></div></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside class=sidebar id=sidebar><div class=sidebar-inner><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target=post-toc-wrap>Table of Contents<li class=sidebar-nav-overview data-target=site-overview-wrap>Overview</ul><section class="site-overview-wrap sidebar-panel"><div class=site-overview><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt class=site-author-image itemprop=image src=/%5Bobject%20Object%5D><p class=site-author-name itemprop=name><p class="site-description motion-element" itemprop=description></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/archives/%7C%7C%20archive> <span class=site-state-item-count>114</span> <span class=site-state-item-name>posts</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/index.html> <span class=site-state-item-count>35</span> <span class=site-state-item-name>categories</span> </a></div><div class="site-state-item site-state-tags"><a href=/tags/index.html> <span class=site-state-item-count>195</span> <span class=site-state-item-name>tags</span> </a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item> <a href=https://github.com/FesianXu target=_blank title=GitHub> <i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class=links-of-author-item> <a href=mailto:FesianXu@gmail.com target=_blank title=E-Mail> <i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class=links-of-author-item> <a href=https://stackoverflow.com/users/7348519/fesianxu target=_blank title=StackOverflow> <i class="fa fa-fw fa-stack-overflow"></i>StackOverflow</a> </span></div></div></section><!--noindex--><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class=post-toc><div class=post-toc-content><ol class=nav><li class="nav-item nav-level-1"><a class=nav-link href=#%E5%89%8D%E8%A8%80><span class=nav-number>1.</span> <span class=nav-text>前言</span></a><li class="nav-item nav-level-1"><a class=nav-link href=#%E8%A7%86%E9%A2%91%E5%88%86%E6%9E%90%E4%B8%8D%E4%BB%85%E6%9C%89%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB><span class=nav-number>2.</span> <span class=nav-text>视频分析不仅有动作识别</span></a><li class="nav-item nav-level-1"><a class=nav-link href=#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E8%A1%A5%E5%85%85%E4%BB%8B%E7%BB%8D><span class=nav-number>3.</span> <span class=nav-text>数据集的补充介绍</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%A4%9A%E6%A8%A1%E6%80%81%E6%95%B0%E6%8D%AE%E9%9B%86><span class=nav-number>3.1.</span> <span class=nav-text>多模态数据集</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#youtube%E6%95%B0%E6%8D%AE%E9%9B%86><span class=nav-number>3.2.</span> <span class=nav-text>YouTube数据集</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#instructional%E6%95%B0%E6%8D%AE%E9%9B%86><span class=nav-number>3.3.</span> <span class=nav-text>Instructional数据集</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link href=#%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AF%AD%E4%B9%89%E8%9E%8D%E5%90%88><span class=nav-number>4.</span> <span class=nav-text>多模态语义融合</span></a><li class="nav-item nav-level-1"><a class=nav-link href=#reference><span class=nav-number>5.</span> <span class=nav-text>Reference</span></a></ol></div></div></section><!--/noindex--></div></aside></div></main><footer class=footer id=footer><div class=footer-inner><div class=copyright>© <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-user"></i> </span><span class=author itemprop=copyrightHolder>FesianXu</span></div><div class=“theme-info”><div class=“powered-by”></div><span class=“post-count”> 该站点文章共377k字，欢迎光临~ </span></div><script async src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><div class=busuanzi-count><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span class=site-uv> <i class="fa fa-user"></i> <span class=busuanzi-value id=busuanzi_value_site_uv></span> </span><span class=site-pv> <i class="fa fa-eye"></i> <span class=busuanzi-value id=busuanzi_value_site_pv></span> </span></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i></div></div><script>if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }</script><script src=/lib/jquery/index.js></script><script src=/lib/fastclick/lib/fastclick.min.js></script><script src=/lib/jquery_lazyload/jquery.lazyload.js></script><script src=/lib/velocity/velocity.min.js></script><script src=/lib/velocity/velocity.ui.min.js></script><script src=/lib/fancybox/source/jquery.fancybox.pack.js></script><script src=/js/src/utils.js></script><script src=/js/src/motion.js></script><script src=/js/src/scrollspy.js></script><script src=/js/src/post-details.js></script><script src=/js/src/bootstrap.js></script><script>(function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');</script><script>// Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.json";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });</script><script src=/live2dw/lib/L2Dwidget.min.js></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7}});</script>