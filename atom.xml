<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>机器学习杂货铺总店</title>
  <icon>https://fesianxu.github.io/icon.png</icon>
  
  <link href="https://fesianxu.github.io/atom.xml" rel="self"/>
  
  <link href="https://fesianxu.github.io/"/>
  <updated>2024-12-21T08:11:18.576Z</updated>
  <id>https://fesianxu.github.io/</id>
  
  <author>
    <name>FesianXu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基于shell的简单好用多进程wrapper</title>
    <link href="https://fesianxu.github.io/2024/12/21/shell-multiprocess-worker-20241221/"/>
    <id>https://fesianxu.github.io/2024/12/21/shell-multiprocess-worker-20241221/</id>
    <published>2024-12-21T08:00:50.000Z</published>
    <updated>2024-12-21T08:11:18.576Z</updated>
    
    
    <summary type="html">&lt;p&gt;基于shell的简单好用多进程wrapper...&lt;/p&gt;</summary>
    
    
    
    <category term="linux使用" scheme="https://fesianxu.github.io/categories/linux%E4%BD%BF%E7%94%A8/"/>
    
    
    <category term="linux 日常使用技巧" scheme="https://fesianxu.github.io/tags/linux-%E6%97%A5%E5%B8%B8%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    <category term="工作实践经验" scheme="https://fesianxu.github.io/tags/%E5%B7%A5%E4%BD%9C%E5%AE%9E%E8%B7%B5%E7%BB%8F%E9%AA%8C/"/>
    
  </entry>
  
  <entry>
    <title>CatLIP，加速2.7倍！采用分类损失的CLIP水准的预训练视觉编码器</title>
    <link href="https://fesianxu.github.io/2024/11/10/catlip-20241110/"/>
    <id>https://fesianxu.github.io/2024/11/10/catlip-20241110/</id>
    <published>2024-11-10T11:19:47.000Z</published>
    <updated>2024-11-10T12:36:30.927Z</updated>
    
    
    <summary type="html">&lt;p&gt;传统的CLIP采用对比学习的方式进行预训练，通常需要汇聚多张节点的多张设备的特征向量以进行打分矩阵的计算，训练速度通常都较慢...&lt;/p&gt;</summary>
    
    
    
    <category term="多模态模型" scheme="https://fesianxu.github.io/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="对比学习" scheme="https://fesianxu.github.io/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="视觉底座模型" scheme="https://fesianxu.github.io/tags/%E8%A7%86%E8%A7%89%E5%BA%95%E5%BA%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="图文预训练" scheme="https://fesianxu.github.io/tags/%E5%9B%BE%E6%96%87%E9%A2%84%E8%AE%AD%E7%BB%83/"/>
    
  </entry>
  
  <entry>
    <title>利用远程服务器实现内网穿透访问jupyter notebook</title>
    <link href="https://fesianxu.github.io/2024/11/06/remote-server-for-jupyter-notebook-20241106/"/>
    <id>https://fesianxu.github.io/2024/11/06/remote-server-for-jupyter-notebook-20241106/</id>
    <published>2024-11-05T16:35:15.000Z</published>
    <updated>2024-11-10T11:40:53.181Z</updated>
    
    
    <summary type="html">&lt;p&gt;穿透内网，访问jupyter notebook...&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习环境搭建" scheme="https://fesianxu.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
    <category term="工作环境搭建" scheme="https://fesianxu.github.io/tags/%E5%B7%A5%E4%BD%9C%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>pytorch手动实现滑动窗口操作，论fold和unfold函数的使用</title>
    <link href="https://fesianxu.github.io/2024/11/06/pytorch-fold-unfold-20241106/"/>
    <id>https://fesianxu.github.io/2024/11/06/pytorch-fold-unfold-20241106/</id>
    <published>2024-11-05T16:31:38.000Z</published>
    <updated>2024-11-10T10:51:17.698Z</updated>
    
    
    <summary type="html">&lt;p&gt;pytorch中&lt;code&gt;fold&lt;/code&gt;和&lt;code&gt;unfold&lt;/code&gt;函数的日常使用方法...&lt;/p&gt;</summary>
    
    
    
    <category term="pytorch使用" scheme="https://fesianxu.github.io/categories/pytorch%E4%BD%BF%E7%94%A8/"/>
    
    
    <category term="pytorch使用教程" scheme="https://fesianxu.github.io/tags/pytorch%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>rsync 用于远程/本地 文件的拷贝（可以实现差量复制）</title>
    <link href="https://fesianxu.github.io/2024/11/06/rsync-cmd-20241106/"/>
    <id>https://fesianxu.github.io/2024/11/06/rsync-cmd-20241106/</id>
    <published>2024-11-05T16:29:17.000Z</published>
    <updated>2024-12-21T08:14:28.688Z</updated>
    
    
    <summary type="html">&lt;p&gt;&lt;code&gt;rsync&lt;/code&gt;的基本使用方法...&lt;/p&gt;</summary>
    
    
    
    <category term="linux使用" scheme="https://fesianxu.github.io/categories/linux%E4%BD%BF%E7%94%A8/"/>
    
    
    <category term="linux常见命令" scheme="https://fesianxu.github.io/tags/linux%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4/"/>
    
  </entry>
  
  <entry>
    <title>解耦多模态大模型中的视觉语义压缩与视觉语义摘要</title>
    <link href="https://fesianxu.github.io/2024/11/06/decouple-compression-abstraction-mllm-20241106/"/>
    <id>https://fesianxu.github.io/2024/11/06/decouple-compression-abstraction-mllm-20241106/</id>
    <published>2024-11-05T16:22:11.000Z</published>
    <updated>2024-11-05T16:26:24.697Z</updated>
    
    
    <summary type="html">&lt;p&gt;在多模态大模型中，视觉连接器大致可以分为压缩型和非圧缩型，其中BLIP2提出的Q-Former
[1] 是压缩型视觉连接器的代表工作之一。在论文 [2]
中，作者对Q-Former的作用提出了质疑和分析，本文进行笔记，希望对读者有所帮助...&lt;/p&gt;</summary>
    
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="视觉连接器" scheme="https://fesianxu.github.io/tags/%E8%A7%86%E8%A7%89%E8%BF%9E%E6%8E%A5%E5%99%A8/"/>
    
    <category term="视觉语义压缩" scheme="https://fesianxu.github.io/tags/%E8%A7%86%E8%A7%89%E8%AF%AD%E4%B9%89%E5%8E%8B%E7%BC%A9/"/>
    
    <category term="视觉语义摘要" scheme="https://fesianxu.github.io/tags/%E8%A7%86%E8%A7%89%E8%AF%AD%E4%B9%89%E6%91%98%E8%A6%81/"/>
    
    <category term="多模态信息提取" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>Flamingo：一种交织图文的视觉语言大模型方法</title>
    <link href="https://fesianxu.github.io/2024/10/18/flamingo-20241018/"/>
    <id>https://fesianxu.github.io/2024/10/18/flamingo-20241018/</id>
    <published>2024-10-17T16:19:36.000Z</published>
    <updated>2024-10-17T16:28:00.746Z</updated>
    
    
    <summary type="html">&lt;p&gt;Flamingo算是DeepMind的多模态融合LLM的一个较老的工作了（2022年），之前粗略读过没来得及及时总结，本次过年笔者重新细读了论文，发现其在50多页的论文中有着不少细节...&lt;/p&gt;</summary>
    
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="视觉连接器" scheme="https://fesianxu.github.io/tags/%E8%A7%86%E8%A7%89%E8%BF%9E%E6%8E%A5%E5%99%A8/"/>
    
    <category term="交织图文训练" scheme="https://fesianxu.github.io/tags/%E4%BA%A4%E7%BB%87%E5%9B%BE%E6%96%87%E8%AE%AD%E7%BB%83/"/>
    
  </entry>
  
  <entry>
    <title>搜索系统中的Learning To Rank模型：GBRank</title>
    <link href="https://fesianxu.github.io/2024/10/18/gbrank-20241018/"/>
    <id>https://fesianxu.github.io/2024/10/18/gbrank-20241018/</id>
    <published>2024-10-17T16:06:59.000Z</published>
    <updated>2024-10-17T16:17:18.424Z</updated>
    
    
    <summary type="html">&lt;p&gt;Learning To
Rank(LTR)模型是对搜索/计算广告/推荐系统中的排序问题进行模型建模的方法，在当前的搜索系统中有着至关重要的作用...&lt;/p&gt;</summary>
    
    
    
    <category term="统计机器学习" scheme="https://fesianxu.github.io/categories/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="搜索系统" scheme="https://fesianxu.github.io/tags/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="gbrank" scheme="https://fesianxu.github.io/tags/gbrank/"/>
    
    <category term="LTR" scheme="https://fesianxu.github.io/tags/LTR/"/>
    
  </entry>
  
  <entry>
    <title>【用户行为学研究】 从用户点击数据中构造隐式反馈</title>
    <link href="https://fesianxu.github.io/2024/10/17/above-skip-20241017/"/>
    <id>https://fesianxu.github.io/2024/10/17/above-skip-20241017/</id>
    <published>2024-10-17T15:54:19.000Z</published>
    <updated>2024-10-17T16:18:42.477Z</updated>
    
    
    <summary type="html">&lt;p&gt;笔者在前文[4]中介绍了LTR模型中常用的GBRank模型，在文章末尾提到了根据用户点击数据构造隐式反馈，从而构建出有序对数据进行训练，因而引出了&lt;code&gt;Skip-Above&lt;/code&gt;这个构建隐式反馈的方法，该方法在文章[1]中提出，作者根据翔实的用户行为学实验和分析，得出了包括&lt;code&gt;Skip-Above&lt;/code&gt;在内的一系列通过点击信号来构建隐式反馈的方法...&lt;/p&gt;</summary>
    
    
    
    <category term="用户行为学" scheme="https://fesianxu.github.io/categories/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%AD%A6/"/>
    
    
    <category term="用户行为学" scheme="https://fesianxu.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%AD%A6/"/>
    
    <category term="隐式反馈" scheme="https://fesianxu.github.io/tags/%E9%9A%90%E5%BC%8F%E5%8F%8D%E9%A6%88/"/>
    
    <category term="用户行为分析" scheme="https://fesianxu.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Alignment与Correspondence，用于量化衡量MLLM中视觉特征的视觉语义对齐与视觉结构程度的方法</title>
    <link href="https://fesianxu.github.io/2024/10/15/ac-score-for-mllm-20241015/"/>
    <id>https://fesianxu.github.io/2024/10/15/ac-score-for-mllm-20241015/</id>
    <published>2024-10-15T15:26:10.000Z</published>
    <updated>2024-10-15T15:36:42.636Z</updated>
    
    
    <summary type="html">&lt;p&gt;在多模态大模型（Multimodal Large Language Model，
MLLM）中，视觉特征就像是人的眼睛，而底座的LLM则像是人的大脑，合适的视觉特征的选择通常都是一个MLLM成功的重要一步...&lt;/p&gt;</summary>
    
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="视觉语义对齐" scheme="https://fesianxu.github.io/tags/%E8%A7%86%E8%A7%89%E8%AF%AD%E4%B9%89%E5%AF%B9%E9%BD%90/"/>
    
    <category term="视觉结构特征" scheme="https://fesianxu.github.io/tags/%E8%A7%86%E8%A7%89%E7%BB%93%E6%9E%84%E7%89%B9%E5%BE%81/"/>
    
  </entry>
  
  <entry>
    <title>SigLIP——采用sigmoid损失的图文预训练方式</title>
    <link href="https://fesianxu.github.io/2024/09/08/sigmoid-language-image-pretrain-20240908/"/>
    <id>https://fesianxu.github.io/2024/09/08/sigmoid-language-image-pretrain-20240908/</id>
    <published>2024-09-08T13:57:57.000Z</published>
    <updated>2024-09-08T14:11:03.110Z</updated>
    
    
    <summary type="html">&lt;p&gt;CLIP中的infoNCE损失是一种对比性损失，在SigLIP这个工作中，作者提出采用非对比性的sigmoid损失，能够&lt;strong&gt;更高效地&lt;/strong&gt;进行图文预训练...&lt;/p&gt;</summary>
    
    
    
    <category term="多模态模型" scheme="https://fesianxu.github.io/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="对比学习" scheme="https://fesianxu.github.io/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="多模态模型" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="计算机视觉" scheme="https://fesianxu.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图文预训练" scheme="https://fesianxu.github.io/tags/%E5%9B%BE%E6%96%87%E9%A2%84%E8%AE%AD%E7%BB%83/"/>
    
    <category term="视觉表征训练" scheme="https://fesianxu.github.io/tags/%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E8%AE%AD%E7%BB%83/"/>
    
    <category term="sigmoid损失" scheme="https://fesianxu.github.io/tags/sigmoid%E6%8D%9F%E5%A4%B1/"/>
    
  </entry>
  
  <entry>
    <title>LexLIP——图片搜索中的多模态稀疏化召回方法</title>
    <link href="https://fesianxu.github.io/2024/07/28/20240728-lexlip/"/>
    <id>https://fesianxu.github.io/2024/07/28/20240728-lexlip/</id>
    <published>2024-07-28T10:39:10.000Z</published>
    <updated>2024-07-28T11:28:16.509Z</updated>
    
    
    <summary type="html">&lt;p&gt;最近笔者在回顾&amp;amp;笔记一些老论文，准备整理下之前看的一篇论文LexLIP，其很适合在真实的图片搜索业务场景中落地，希望笔记能给读者带来启发。&lt;/p&gt;</summary>
    
    
    
    <category term="多模态检索" scheme="https://fesianxu.github.io/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A3%80%E7%B4%A2/"/>
    
    
    <category term="对比学习" scheme="https://fesianxu.github.io/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="论文极速读" scheme="https://fesianxu.github.io/tags/%E8%AE%BA%E6%96%87%E6%9E%81%E9%80%9F%E8%AF%BB/"/>
    
    <category term="多模态检索" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A3%80%E7%B4%A2/"/>
    
    <category term="视觉稀疏化" scheme="https://fesianxu.github.io/tags/%E8%A7%86%E8%A7%89%E7%A8%80%E7%96%8F%E5%8C%96/"/>
    
    <category term="图片搜索" scheme="https://fesianxu.github.io/tags/%E5%9B%BE%E7%89%87%E6%90%9C%E7%B4%A2/"/>
    
    <category term="信息召回（第一阶段检索）" scheme="https://fesianxu.github.io/tags/%E4%BF%A1%E6%81%AF%E5%8F%AC%E5%9B%9E%EF%BC%88%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%E6%A3%80%E7%B4%A2%EF%BC%89/"/>
    
  </entry>
  
  <entry>
    <title>可微分检索索引（Differential Search Index， DSI）</title>
    <link href="https://fesianxu.github.io/2024/07/14/20240714-differential-search-index/"/>
    <id>https://fesianxu.github.io/2024/07/14/20240714-differential-search-index/</id>
    <published>2024-07-14T14:55:16.000Z</published>
    <updated>2024-07-14T15:19:06.207Z</updated>
    
    
    <summary type="html">&lt;p&gt;最近从朋友处得知了DSI这个概念，所谓的可微分检索索引DSI，就是通过语言模型将检索过程中的索引和召回阶段端到端地融合在一起，输入&lt;code&gt;query&lt;/code&gt;模型直接输出&lt;code&gt;docid&lt;/code&gt;，笔者今日抽空看了下原论文，简单笔记下，希望对各位读者有所帮助...&lt;/p&gt;</summary>
    
    
    
    <category term="检索系统" scheme="https://fesianxu.github.io/categories/%E6%A3%80%E7%B4%A2%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="检索系统" scheme="https://fesianxu.github.io/tags/%E6%A3%80%E7%B4%A2%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="端到端检索" scheme="https://fesianxu.github.io/tags/%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%A3%80%E7%B4%A2/"/>
    
    <category term="DSI" scheme="https://fesianxu.github.io/tags/DSI/"/>
    
    <category term="LLM" scheme="https://fesianxu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>基于CLIP特征的多模态大模型中的视觉短板问题</title>
    <link href="https://fesianxu.github.io/2024/07/06/20240706-visual-shortcome-mllm/"/>
    <id>https://fesianxu.github.io/2024/07/06/20240706-visual-shortcome-mllm/</id>
    <published>2024-07-06T10:01:48.000Z</published>
    <updated>2024-07-06T10:40:10.245Z</updated>
    
    
    <summary type="html">&lt;p&gt;如今的大多数多模态大模型，其视觉输入侧采用的视觉编码器，都是依照CLIP的训练方式，采用大规模对比学习进行训练的。在论文
[1]
中，作者发现CLIP特征具有某些视觉短板，从而导致基于此的MLLM也受到了影响。作者观察到，在一些简单直接（不需要复杂推理）的问题上，MLLM似乎并不能很好解决...&lt;/p&gt;</summary>
    
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="对比学习" scheme="https://fesianxu.github.io/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="CLIP" scheme="https://fesianxu.github.io/tags/CLIP/"/>
    
    <category term="自监督视觉模型" scheme="https://fesianxu.github.io/tags/%E8%87%AA%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="论文极速读" scheme="https://fesianxu.github.io/tags/%E8%AE%BA%E6%96%87%E6%9E%81%E9%80%9F%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>万字浅析视频搜索系统中的多模态能力建设</title>
    <link href="https://fesianxu.github.io/2024/06/30/video-retrieval-multimodal-20240630/"/>
    <id>https://fesianxu.github.io/2024/06/30/video-retrieval-multimodal-20240630/</id>
    <published>2024-06-30T13:36:32.000Z</published>
    <updated>2024-06-30T14:44:13.928Z</updated>
    
    
    <summary type="html">&lt;p&gt;视频搜索是天然的富媒体检索场景，视觉信息占据了视频的一大部分信息量，在视频搜索系统中引入多模态能力，对于提高整个系统的能力天花板至关重要。本文将对在视频搜索系统中落地多模态能力（特别是视觉）进行讨论，同时为了让部分无相关背景的读者补充一些背景知识，笔者将会对典型的搜索系统进行介绍...&lt;/p&gt;</summary>
    
    
    
    <category term="多模态检索" scheme="https://fesianxu.github.io/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A3%80%E7%B4%A2/"/>
    
    
    <category term="多模态检索" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A3%80%E7%B4%A2/"/>
    
    <category term="视频搜索" scheme="https://fesianxu.github.io/tags/%E8%A7%86%E9%A2%91%E6%90%9C%E7%B4%A2/"/>
    
    <category term="多模态能力落地" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%83%BD%E5%8A%9B%E8%90%BD%E5%9C%B0/"/>
    
    <category term="多模态语义对齐" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AF%AD%E4%B9%89%E5%AF%B9%E9%BD%90/"/>
    
    <category term="多模态信息融合" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E8%9E%8D%E5%90%88/"/>
    
  </entry>
  
  <entry>
    <title>BLIP2——采用Q-Former融合视觉语义与LLM能力的方法</title>
    <link href="https://fesianxu.github.io/2024/06/12/blip2-20240612/"/>
    <id>https://fesianxu.github.io/2024/06/12/blip2-20240612/</id>
    <published>2024-06-11T16:29:10.000Z</published>
    <updated>2024-06-11T16:34:26.669Z</updated>
    
    
    <summary type="html">&lt;p&gt;大规模语言模型（Large Language
Model,LLM）是当前的当红炸子鸡，展现出了强大的逻辑推理，语义理解能力，而视觉作为人类最为主要的感知世界的手段，亟待和LLM进行融合，形成多模态大规模语言模型（Multimodal
LLM,
MLLM），BLIP-2这篇文章利用已经充分训练好的图片编码器和LLM模型，通过Q-Former巧妙地融合在一起，在引入少量待学习参数的同时，取得了显著的效果。本文将对BLIP2进行笔记和笔者个人感想纪录，希望对诸位读者有所帮助。&lt;/p&gt;</summary>
    
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="多模态融合" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%9E%8D%E5%90%88/"/>
    
    <category term="大语言模型" scheme="https://fesianxu.github.io/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>【论文极速读】MetaLM, 一种融合因果语言模型和非因果语言模型的方法</title>
    <link href="https://fesianxu.github.io/2024/06/08/metalm-20240608/"/>
    <id>https://fesianxu.github.io/2024/06/08/metalm-20240608/</id>
    <published>2024-06-08T09:11:25.000Z</published>
    <updated>2024-06-08T09:32:29.854Z</updated>
    
    
    <summary type="html">&lt;p&gt;熟悉笔者的读者想必都知道，最近我在恶补一些经典的LLM工作，之前也精读过MetaLM这个工作但是没有及时笔记，现在已经有些遗忘了，因此在过年期间复习了下，在此笔记希望对诸位有所帮助。&lt;/p&gt;</summary>
    
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="多模态融合" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%9E%8D%E5%90%88/"/>
    
    <category term="大语言模型" scheme="https://fesianxu.github.io/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="因果语言模型" scheme="https://fesianxu.github.io/tags/%E5%9B%A0%E6%9E%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="非因果语言模型" scheme="https://fesianxu.github.io/tags/%E9%9D%9E%E5%9B%A0%E6%9E%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>Kosmos-2, 在多模态大语言模型中引入基准和指代能力</title>
    <link href="https://fesianxu.github.io/2024/06/08/kosmos-v2-20240608/"/>
    <id>https://fesianxu.github.io/2024/06/08/kosmos-v2-20240608/</id>
    <published>2024-06-08T09:11:06.000Z</published>
    <updated>2024-06-08T09:23:11.268Z</updated>
    
    
    <summary type="html">&lt;p&gt;之前笔者在博文中介绍过kosmos-1模型
[1]，该模型脱胎于MetaLM采用『因果语言模型作为通用任务接口』的思想，采用了多种形式的多模态数据进行训练得到。而在本文将要介绍的kosmos-2中，作者则尝试引入了基准（grounding）和指代（referring）能力，使得多模态大语言模型的人机交互形式更加友好、灵活和多样。&lt;/p&gt;</summary>
    
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="多模态融合" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%9E%8D%E5%90%88/"/>
    
    <category term="大语言模型" scheme="https://fesianxu.github.io/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>Kosmos-1, 通用接口架构下的多模态大语言模型</title>
    <link href="https://fesianxu.github.io/2024/06/08/kosmos-v1-20240608/"/>
    <id>https://fesianxu.github.io/2024/06/08/kosmos-v1-20240608/</id>
    <published>2024-06-08T09:10:55.000Z</published>
    <updated>2024-06-08T09:17:06.586Z</updated>
    
    
    <summary type="html">&lt;p&gt;在大规模语言模型（Large Language Model,
LLM）看似要带来新一番人工智能变革浪潮之际，越来越多尝试以LLM作为通用接口去融入各种任务的工作，之前我们在[2]中曾经对其进行过简单介绍，比如尝试用LLM去控制浏览器、搜索引擎甚至是机械臂等。本文介绍的工作kosmos-1是LLM与多模态信号结合的一种尝试，对笔者有所启发，在此给大家进行推荐。&lt;/p&gt;</summary>
    
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="多模态融合" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%9E%8D%E5%90%88/"/>
    
    <category term="大语言模型" scheme="https://fesianxu.github.io/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>【论文极速读】 指令微调BLIP, 一种对指令微调敏感的Q-Former设计</title>
    <link href="https://fesianxu.github.io/2024/06/08/instruct-blip-20240608/"/>
    <id>https://fesianxu.github.io/2024/06/08/instruct-blip-20240608/</id>
    <published>2024-06-08T08:29:46.000Z</published>
    <updated>2024-06-08T08:53:35.698Z</updated>
    
    
    <summary type="html">&lt;p&gt;之前笔者在[1]中曾经介绍过BLIP2，其采用Q-Former的方式融合了多模态视觉信息和LLM，本文作者想要简单介绍一个在BLIP2的基础上进一步加强了图文指令微调能力的工作——InstructBLIP，希望对诸位读者有所帮助。&lt;/p&gt;</summary>
    
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="多模态大模型" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="多模态融合" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%9E%8D%E5%90%88/"/>
    
    <category term="多模态" scheme="https://fesianxu.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
    <category term="指令微调" scheme="https://fesianxu.github.io/tags/%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83/"/>
    
  </entry>
  
</feed>
