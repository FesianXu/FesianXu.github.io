<!doctypehtml><html class="theme-next muse use-motion" lang=zh-CN><meta charset=UTF-8><meta content=IE=edge http-equiv=X-UA-Compatible><meta content=width=device-width,initial-scale=1,maximum-scale=1 name=viewport><meta content=#222 name=theme-color><meta content=no-transform http-equiv=Cache-Control><meta content=no-siteapp http-equiv=Cache-Control><link href=/lib/fancybox/source/jquery.fancybox.css?v=2.1.5 rel=stylesheet><link href=/lib/font-awesome/css/font-awesome.min.css?v=4.6.2 rel=stylesheet><link href=/css/main.css?v=5.1.4 rel=stylesheet><link href=/images/apple-touch-icon-next.png?v=5.1.4 rel=apple-touch-icon sizes=180x180><link href=/images/favicon-32x32-next.png?v=5.1.4 rel=icon sizes=32x32 type=image/png><link href=/images/favicon-16x16-next.png?v=5.1.4 rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg?v=5.1.4 rel=mask-icon><meta content=多模态大模型,视觉连接器,交织图文训练, name=keywords><link href=/atom.xml rel=alternate title=机器学习杂货铺总店 type=application/atom+xml><meta content=Flamingo算是DeepMind的多模态融合LLM的一个较老的工作了（2022年），之前粗略读过没来得及及时总结，本次过年笔者重新细读了论文，发现其在50多页的论文中有着不少细节... name=description><meta content=article property=og:type><meta content=Flamingo：一种交织图文的视觉语言大模型方法 property=og:title><meta content=https://fesianxu.github.io/2024/10/18/flamingo-20241018/index.html property=og:url><meta content=机器学习杂货铺总店 property=og:site_name><meta content=Flamingo算是DeepMind的多模态融合LLM的一个较老的工作了（2022年），之前粗略读过没来得及及时总结，本次过年笔者重新细读了论文，发现其在50多页的论文中有着不少细节... property=og:description><meta content=zh_CN property=og:locale><meta content=https://fesianxu.github.io/2024/10/18/flamingo-20241018/qrcode.png property=og:image><meta content=https://fesianxu.github.io/2024/10/18/flamingo-20241018/imgs/fig1_framework.png property=og:image><meta content=https://fesianxu.github.io/2024/10/18/flamingo-20241018/imgs/fig2_pr_xattn.png property=og:image><meta content=https://fesianxu.github.io/2024/10/18/flamingo-20241018/imgs/fig3_alpha_gated.png property=og:image><meta content=https://fesianxu.github.io/2024/10/18/flamingo-20241018/imgs/fig4_web_interleave_image_text.png property=og:image><meta content=https://fesianxu.github.io/2024/10/18/flamingo-20241018/imgs/fig5_train_m3w.png property=og:image><meta content=https://fesianxu.github.io/2024/10/18/flamingo-20241018/imgs/fig6_ablation.png property=og:image><meta content=2024-10-17T16:19:36.000Z property=article:published_time><meta content=2024-10-17T16:28:00.746Z property=article:modified_time><meta content=FesianXu property=article:author><meta content=多模态大模型 property=article:tag><meta content=视觉连接器 property=article:tag><meta content=交织图文训练 property=article:tag><meta content=summary name=twitter:card><meta content=https://fesianxu.github.io/2024/10/18/flamingo-20241018/qrcode.png name=twitter:image><script id=hexo.configurations>var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };</script><link href=https://FesianXu.github.io/2024/10/18/flamingo-20241018/ rel=canonical><title>Flamingo：一种交织图文的视觉语言大模型方法 | 机器学习杂货铺总店</title><meta content="Hexo 7.3.0" name=generator><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}</style><body itemscope itemtype=http://schema.org/WebPage lang=zh-CN><div class="container sidebar-position-left page-post-detail"><div class=headband></div><header class=header id=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-wrapper><div class=site-meta><div class=custom-logo-site-title><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <span class=site-title>机器学习杂货铺总店</span> <span class=logo-line-after><i></i></span> </a></div><p class=site-subtitle></div><div class=site-nav-toggle><button><span class=btn-bar></span> <span class=btn-bar></span> <span class=btn-bar></span></button></div></div><nav class=site-nav><ul class=menu id=menu><li class="menu-item menu-item-home"><a href=/ rel=section> <i class="menu-item-icon fa fa-fw fa-home"></i> <br> Home </a><li class="menu-item menu-item-about"><a href=/about/ rel=section> <i class="menu-item-icon fa fa-fw fa-user"></i> <br> About </a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section> <i class="menu-item-icon fa fa-fw fa-tags"></i> <br> Tags </a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section> <i class="menu-item-icon fa fa-fw fa-th"></i> <br> Categories </a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section> <i class="menu-item-icon fa fa-fw fa-archive"></i> <br> Archives </a><li class="menu-item menu-item-search"><a class=popup-trigger href=javascript:;> <i class="menu-item-icon fa fa-search fa-fw"></i> <br> Search </a></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon> <i class="fa fa-search"></i> </span><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span><div class=local-search-input-wrapper><input autocomplete=off id=local-search-input placeholder=Searching... spellcheck=false></div></div><div id=local-search-result></div></div></div></nav></div></header><main class=main id=main><div class=main-inner><div class=content-wrap><div class=content id=content><div class=posts-expand id=posts><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://FesianXu.github.io/2024/10/18/flamingo-20241018/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/%5Bobject%20Object%5D itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=机器学习杂货铺总店 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>Flamingo：一种交织图文的视觉语言大模型方法</h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>Posted on</span> <time itemprop="dateCreated datePublished" title="Post created" datetime=2024-10-18T00:19:36+08:00> 2024-10-18 </time> </span><span class=post-category> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-folder-o"></i> </span> <span class=post-meta-item-text>In</span> <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/ itemprop=url rel=index> <span itemprop=name>多模态大模型</span> </a> </span> </span><span class=post-meta-divider>|</span><span class=page-pv><i class="fa fa-file-o"></i> <span class=busuanzi-value id=busuanzi_value_page_pv></span> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-file-word-o"></i> </span><span class=post-meta-item-text>Words count in article:</span><span title="Words count in article"> 22k 字 </span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span class=post-meta-item-text>Reading time ≈</span><span title="Reading time"> 1:18 分钟 </span></div></div></header><div class=post-body itemprop=articleBody><p>Flamingo算是DeepMind的多模态融合LLM的一个较老的工作了（2022年），之前粗略读过没来得及及时总结，本次过年笔者重新细读了论文，发现其在50多页的论文中有着不少细节... <span id=more></span><div align=right>FesianXu 20240215 at Baidu Search Team</div><h1 id=前言>前言</h1><p>Flamingo算是DeepMind的多模态融合LLM的一个较老的工作了（2022年），之前粗略读过没来得及及时总结，本次过年笔者重新细读了论文，发现其在50多页的论文中有着不少细节，本文对该工作进行读后感笔记，希望对诸位读者有所帮助。如有谬误请见谅并联系指出，本文遵守CC 4.0 BY-SA版权协议，转载请联系作者并注明出处，谢谢。<p><span class="math inline"><mjx-container class=MathJax jax=SVG><svg style="vertical-align: -0.075ex;" viewbox="0 -683 833 716" focusable=false height=1.62ex role=img width=1.885ex xmlns=http://www.w3.org/2000/svg><g fill=currentColor stroke=currentColor stroke-width=0 transform=scale(1,-1)><g data-mml-node=math><g data-mml-node=mi><path d="M46 676Q46 679 51 683H781Q786 679 786 676Q786 674 617 326T444 -26Q439 -33 416 -33T388 -26Q385 -22 216 326T46 676ZM697 596Q697 597 445 597T193 596Q195 591 319 336T445 80L697 596Z" data-c=2207 /></g></g></g></svg></mjx-container></span> 联系方式<p>e-mail: FesianXu@gmail.com<p>github: https://github.com/FesianXu<p>知乎专栏: 计算机视觉/计算机图形理论与应用(https://www.zhihu.com/column/c_1265262560611299328)<p><img src=/2024/10/18/flamingo-20241018/qrcode.png><hr><p>之前笔者在介绍BLIP2的博文[1]中，曾经介绍过采用Q-Former融合视觉语义向量和LLM的方法，BLIP2工作中由于只采用了图文对（Image-Text Pair）数据，因此其in-context能力欠缺，few-shot效果不佳。而在Flamingo [2] 这个工作中，作者从互联网数据中收集了大量的图文交织（Image-Text Interleaving）数据，这为Flamingo提供few-shot、in-context能力提供了基础保障，虽然Flamingo中采用Perceiver Resampler和Gated cross-attention的方法融合多模态信息目前不是主流（主流还是Q-Former），但其论文中提到的数据构建方式，模型结构消融实验等仍然能提供很多有意义的参考。我们接下来开始对Flamingo进行介绍。<p>Flamingo [2] 和 BLIP2 [3] 都是尝试将已预训练好的视觉特征编码器（如ViT、Resnet等）和已预训练好的大语言模型（LLama、OPT等）进行结合的工作，从而使得大语言模型可以交织文本和视觉进行输入（如图片、视频等），最终输出文本，我们称之为MLLM（多模态大语言模型），读者可在博文 [1] 中得到更多相关的背景信息，在此不再累述。Flamingo采用了所谓的<strong>感知重采样</strong>（Perceiver Resampler）技术和<strong>门控交叉注意力</strong>技术（Gated Cross-Attention）进行视觉多模态信息和LLM的融合，整体结构如Fig 1.所示，其中视觉编码器和LLM都是固定参数而不在训练中更新，感知重采样器将变长的视觉向量转换成定长的多模态语义向量，通过门控注意力单元将信息融入固定的LLM中，最终实现输入中可混合多模态信息而输出文本信息。 <img src=/2024/10/18/flamingo-20241018/imgs/fig1_framework.png><div align=center><b> Fig 1. Flamingo的框架图，主要由视觉编码器（vision encoder）、感知重采样器（perceiver resampler）、LLM和交织在LLM中的门控交叉注意力层（gated xattn-dense）组成。 </b></div><p>其中感知重采样器和门控注意力单元的结构如Fig 2所示，其中的视觉编码器采用NFNet（NormalizerFree ResNet），作者先在图文对数据上采用CLIP的方式对NFNet进行预训练，随后进行参数固定。如果视觉端输入是视频，则按照1 fps进行采样后将<span class="math inline"><mjx-container class=MathJax jax=SVG><svg style="vertical-align: 0;" viewbox="0 -683 888 683" focusable=false height=1.545ex role=img width=2.009ex xmlns=http://www.w3.org/2000/svg><g fill=currentColor stroke=currentColor stroke-width=0 transform=scale(1,-1)><g data-mml-node=math><g data-mml-node=mi><path d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" data-c=1D441 /></g></g></g></svg></mjx-container></span>帧进行视觉特征提取（若是图片输入，则N=1），注意到此时position embedding按照帧粒度组织，即是统一帧的不同patch共用一个position embedding以建模帧间序列信息。尔后对多帧的特征进行展开、拼接，作为transformer的k和v，而采用一个可学习的query向量作为transformer的q输入，这个思路可参考博文 [1]，不在此展开，具体伪代码可见Code 1。感知重采样机制的一个好处就是，可以将变长的视频输入转变为定长的输入，此处定长的输入长度为64。<p>门控注意力单元的设计，则是在原先固定的LLM结构的每一层基础上叠加了门控单元，门控单元由交叉注意力机制和门控结构、FFW交替组成，其中交叉注意力的k和v都是感知重采样器的输出，而q则是文本输入。为了保证在训练初始阶段模型和原先的LLM不至于偏差太远，作者采用了门控机制，具体来说就是将新层的输出乘上一个可学习的<span class="math inline"><mjx-container class=MathJax jax=SVG><svg style="vertical-align: -0.566ex;" viewbox="0 -750 3419 1000" focusable=false height=2.262ex role=img width=7.735ex xmlns=http://www.w3.org/2000/svg><g fill=currentColor stroke=currentColor stroke-width=0 transform=scale(1,-1)><g data-mml-node=math><g data-mml-node=mi><path d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" data-c=74 /><path d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" data-c=61 transform=translate(389,0) /><path d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" data-c=6E transform=translate(889,0) /><path d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 124T102 167T103 217T103 272T103 329Q103 366 103 407T103 482T102 542T102 586T102 603Q99 622 88 628T43 637H25V660Q25 683 27 683L37 684Q47 685 66 686T103 688Q120 689 140 690T170 693T181 694H184V367Q244 442 328 442Q451 442 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" data-c=68 transform=translate(1445,0) /></g><g data-mml-node=mo transform=translate(2001,0)><path data-c=2061 /></g><g data-mml-node=mo transform=translate(2001,0)><path d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" data-c=28 /></g><g data-mml-node=mi transform=translate(2390,0)><path d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z" data-c=1D6FC /></g><g data-mml-node=mo transform=translate(3030,0)><path d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" data-c=29 /></g></g></g></svg></mjx-container></span>，将LLM的原先输入与其加和，只需要在初始化时候将<span class="math inline"><mjx-container class=MathJax jax=SVG><svg style="vertical-align: -0.186ex;" viewbox="0 -666 2473.6 748" focusable=false height=1.692ex role=img width=5.596ex xmlns=http://www.w3.org/2000/svg><g fill=currentColor stroke=currentColor stroke-width=0 transform=scale(1,-1)><g data-mml-node=math><g data-mml-node=mi><path d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z" data-c=1D6FC /></g><g data-mml-node=mo transform=translate(917.8,0)><path d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" data-c=3D /></g><g data-mml-node=mn transform=translate(1973.6,0)><path d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" data-c=30 /></g></g></g></svg></mjx-container></span>即可确保初始化时候和原先LLM无太大偏差。作者对在训练过程中每一LM层的<span class="math inline"><mjx-container class=MathJax jax=SVG><svg style="vertical-align: -0.025ex;" viewbox="0 -442 640 453" focusable=false height=1.025ex role=img width=1.448ex xmlns=http://www.w3.org/2000/svg><g fill=currentColor stroke=currentColor stroke-width=0 transform=scale(1,-1)><g data-mml-node=math><g data-mml-node=mi><path d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z" data-c=1D6FC /></g></g></g></svg></mjx-container></span>变化进行了可视化，见Fig 3.可发现两个规律，第一随着层数加深，门控值则更大，第二随着训练过程，门控值也逐渐变大，这个倒是符合我们的认识，浅层提取基础特征而深层则更加富有语义信息，因此在深层中的门控更大有利于引入更多的视觉语义信息。 <img src=/2024/10/18/flamingo-20241018/imgs/fig2_pr_xattn.png><div align=center><b> Fig 2. Flamingo中采用的感知重采样器和门控交叉注意力模型结构。 </b></div><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br></pre><td class=code><pre><span class=line><span class=keyword>def</span> <span class="title function_">perceiver_resampler</span>(<span class=params></span></span><br><span class=line><span class=params>	x_f, <span class=comment># The [T, S, d] visual features (T=time, S=space)</span></span></span><br><span class=line><span class=params>	time_embeddings, <span class=comment># The [T, 1, d] time pos embeddings.</span></span></span><br><span class=line><span class=params>	x, <span class=comment># R learned latents of shape [R, d]</span></span></span><br><span class=line><span class=params>	num_layers, <span class=comment># Number of layers</span></span></span><br><span class=line><span class=params></span>):</span><br><span class=line>	<span class=string>"""The Perceiver Resampler model."""</span></span><br><span class=line>	<span class=comment># Add the time position embeddings and flatten.</span></span><br><span class=line>	x_f = x_f + time_embeddings</span><br><span class=line>	x_f = flatten(x_f) <span class=comment># [T, S, d] -> [T * S, d]</span></span><br><span class=line>	<span class=comment># Apply the Perceiver Resampler layers.</span></span><br><span class=line>	<span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(num_layers):</span><br><span class=line>		<span class=comment># Attention.</span></span><br><span class=line>		x = x + attention_i(q=x, kv=concat([x_f, x]))</span><br><span class=line>		<span class=comment># Feed forward.</span></span><br><span class=line>		x = x + ffw_i(x)</span><br><span class=line>	<span class=keyword>return</span> x</span><br><span class=line></span><br><span class=line><span class=keyword>def</span> <span class="title function_">gated_xattn_dense</span>(<span class=params></span></span><br><span class=line><span class=params>	y, <span class=comment># input language features</span></span></span><br><span class=line><span class=params>	x, <span class=comment># input visual features</span></span></span><br><span class=line><span class=params>	alpha_xattn, <span class=comment># xattn gating parameter – init at 0.</span></span></span><br><span class=line><span class=params>	alpha_dense, <span class=comment># ffw gating parameter – init at 0.</span></span></span><br><span class=line><span class=params></span>):</span><br><span class=line>	<span class=string>"""Applies a GATED XATTN-DENSE layer."""</span></span><br><span class=line>	<span class=comment># 1. Gated Cross Attention</span></span><br><span class=line>	y = y + tanh(alpha_xattn) * attention(q=y, kv=x)</span><br><span class=line>	<span class=comment># 2. Gated Feed Forward (dense) Layer</span></span><br><span class=line>	y = y + tanh(alpha_dense) * ffw(y)</span><br><span class=line>	<span class=comment># Regular self-attention + FFW on language</span></span><br><span class=line>	y = y + frozen_attention(q=y, kv=y)</span><br><span class=line>	y = y + frozen_ffw(y)</span><br><span class=line>	<span class=keyword>return</span> y <span class=comment># output visually informed language features</span></span><br></pre></table></figure><div align=center><b> Code 1. 感知重采样器和门控交叉注意力单元的伪代码。 </b></div><p><img src=/2024/10/18/flamingo-20241018/imgs/fig3_alpha_gated.png><div align=center><b> Fig 3. 注意力层中和FFW的门控值在不同层的变化趋势。 </b></div><p>说完了模型结构上的改动，我们还需要关注到本工作中的数据构建，在本工作中，作者不仅仅构建了图文对数据（LTIP），而且还构建了视频对数据（VTP）和图文交织数据。图文交织数据（M3W: Interleaved image and text dataset）指的是图片和文本进行多次交织组成的数据，图片会穿插在文本上下文中，而不是简单的图文一对一的关系数据。作者通过解析大概4.3千万个网页的DOM，构建了图文交织数据，如Fig 4.所示，图片穿插在了文章上下文中，而上文和下文可能和该图片都由语义关联。<p><img src=/2024/10/18/flamingo-20241018/imgs/fig4_web_interleave_image_text.png><div align=center><b> Fig 4. 来自于网页上的图文交织数据示例，注意到相关的图片会穿插在文本之间，上文和下文都可能和该图片有语义关联。原网页来自 [4]。 </b></div><p>怎么对图文交织数据进行建模也是一个值得关注的点，如Fig 5 (b) 所示，一张内嵌到文本中的图片可能和上文或者下文或者两者都产生语义关联，在Flamingo中作者选择按照概率<span class="math inline"><mjx-container class=MathJax jax=SVG><svg style="vertical-align: -0.439ex;" viewbox="0 -442 1999.5 636" focusable=false height=1.439ex role=img width=4.524ex xmlns=http://www.w3.org/2000/svg><g fill=currentColor stroke=currentColor stroke-width=0 transform=scale(1,-1)><g data-mml-node=math><g data-mml-node=msub><g data-mml-node=mi><path d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" data-c=1D45D /></g><g transform="translate(536,-150) scale(0.707)" data-mjx-texclass=ORD data-mml-node=TeXAtom><g data-mml-node=mi><path d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" data-c=1D45B /></g><g data-mml-node=mi transform=translate(600,0)><path d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" data-c=1D452 /></g><g data-mml-node=mi transform=translate(1066,0)><path d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" data-c=1D465 /></g><g data-mml-node=mi transform=translate(1638,0)><path d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" data-c=1D461 /></g></g></g></g></g></svg></mjx-container></span>采样后续文本作为成对文本，亦或者反过来选择前继文本。从图文交织数据中可以组成多个成对数据，如Fig 5 (a)所示，此时通过门控交叉注意力单元中的掩膜设置，可以同时对该图文交织数据中出现的多个成对数据进行建模，具体原理见 [5]。当然，这里对图文交织数据的应用会比较朴素，只考虑以某个图片为中心的局部单向语义信息，而没有考虑到全局信息的建模，相当于还是简单将图文交织数据去局部组织图文对数据进行训练。<p>在交叉注意力单元中采用这种方式，虽然一次性只能让图片直接关注到一个相关联的文本，但是通过后续的LM单元的自注意力模块，能同时建模任意数量的图片输入和文本输入，实现图文交织数据作为输入的对话，当然也就能支持BLIP2所欠缺的few-shot功能了。这里的做法，按照笔者的认识，相当于就是交叉注意力单元只负责建模图文局部的语义对齐，而图文交织数据全局的信息对齐则由紧接着的LM完成。 <img src=/2024/10/18/flamingo-20241018/imgs/fig5_train_m3w.png><div align=center><b> Fig 5. 在Flamingo中应用图文交织型数据的方法，由于嵌入到文本中的图片可能和上文、下文产生语义关联，在本工作中采用按一定概率的方式采样后续文本进行应用。 </b></div><p>Flamingo的效果从benchmark测试看是能吊打很多few-shot和zero-shot数据集的sota方法的，受限于篇幅笔者不会展开。在文中作者做了很多坚实的消融实验，去验证Flamingo的各种设计的效果，如Fig 6.所示，主要对几点进行了消融： 1. 是否采用全量数据？ 特别是对M3W图文交织数据的有无进行了消融，我们发现图文交织数据能提供大约17%的提升。 2. 是否采用门控机制？实验证明采用门控机制能带来月8%的提升。 3. 采用交叉注意力层的频率？实验证明每一层都引入门控交叉注意力层效果是最好的。 4. 是否采用感知重采样单元引入视觉信息？实验证明该设计能带来约4%的提升。 5. 视觉编码器的选择同样对结果影响巨大。 6. 是否固定LLM的参数？实验证明固定LLM反而能带来最好的效果，而让LLM随着训练一起进行（会采用massive text数据集一起训练）反而效果会差8%左右，笔者估计是训练过程需要平衡多个目标导致的，如何让LLM也能训练起来可能也是一个值得关注的点。 <img src=/2024/10/18/flamingo-20241018/imgs/fig6_ablation.png><div align=center><b> Fig 6. 对Flamingo的消融实验。 </b></div><h1 id=reference>Reference</h1><p>[1]. https://blog.csdn.net/LoseInVain/article/details/136013909， 《BLIP2——采用Q-Former融合视觉语义与LLM能力的方法》<p>[2]. Alayrac, J. B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., ... & Simonyan, K. (2022). Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35, 23716-23736. aka Flamingo<p>[3]. Li, Junnan, Dongxu Li, Silvio Savarese, and Steven Hoi. “Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.” arXiv preprint arXiv:2301.12597 (2023). aka BLIP2<p>[4]. https://baijiahao.baidu.com/s?id=1761390872940868294&wfr=spider&for=pc<p>[5]. https://blog.csdn.net/LoseInVain/article/details/119530520</div><div><ul class=post-copyright><li class=post-copyright-author><strong>Post author:</strong> FesianXu<li class=post-copyright-link><strong>Post link:</strong> <a href=https://fesianxu.github.io/2024/10/18/flamingo-20241018/ title=Flamingo：一种交织图文的视觉语言大模型方法>https://fesianxu.github.io/2024/10/18/flamingo-20241018/</a><li class=post-copyright-license><strong>Copyright Notice: </strong> All articles in this blog are licensed under <a rel="external nofollow" href=https://creativecommons.org/licenses/by-nc-sa/3.0/ target=_blank>CC BY-NC-SA 3.0</a> unless stating additionally.</ul></div><footer class=post-footer><div class=post-tags><a href=/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/ rel=tag># 多模态大模型</a><a href=/tags/%E8%A7%86%E8%A7%89%E8%BF%9E%E6%8E%A5%E5%99%A8/ rel=tag># 视觉连接器</a><a href=/tags/%E4%BA%A4%E7%BB%87%E5%9B%BE%E6%96%87%E8%AE%AD%E7%BB%83/ rel=tag># 交织图文训练</a></div><div class=post-nav><div class="post-nav-next post-nav-item"><a title="搜索系统中的Learning To Rank模型：GBRank" href=/2024/10/18/gbrank-20241018/ rel=next> <i class="fa fa-chevron-left"></i> 搜索系统中的Learning To Rank模型：GBRank </a></div><span class=post-nav-divider></span><div class="post-nav-prev post-nav-item"><a href=/2024/11/06/decouple-compression-abstraction-mllm-20241106/ rel=prev title=解耦多模态大模型中的视觉语义压缩与视觉语义摘要> 解耦多模态大模型中的视觉语义压缩与视觉语义摘要 <i class="fa fa-chevron-right"></i> </a></div></div></footer></div></article><div class=post-spread></div></div></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside class=sidebar id=sidebar><div class=sidebar-inner><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target=post-toc-wrap>Table of Contents<li class=sidebar-nav-overview data-target=site-overview-wrap>Overview</ul><section class="site-overview-wrap sidebar-panel"><div class=site-overview><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt class=site-author-image itemprop=image src=/%5Bobject%20Object%5D><p class=site-author-name itemprop=name><p class="site-description motion-element" itemprop=description></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/archives/%7C%7C%20archive> <span class=site-state-item-count>124</span> <span class=site-state-item-name>posts</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/index.html> <span class=site-state-item-count>40</span> <span class=site-state-item-name>categories</span> </a></div><div class="site-state-item site-state-tags"><a href=/tags/index.html> <span class=site-state-item-count>221</span> <span class=site-state-item-name>tags</span> </a></div></nav><div class="feed-link motion-element"><a href=/atom.xml rel=alternate> <i class="fa fa-rss"></i> RSS </a></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a href=https://github.com/FesianXu target=_blank title=GitHub> <i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class=links-of-author-item> <a href=mailto:FesianXu@gmail.com target=_blank title=E-Mail> <i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class=links-of-author-item> <a href=https://stackoverflow.com/users/7348519/fesianxu target=_blank title=StackOverflow> <i class="fa fa-fw fa-stack-overflow"></i>StackOverflow</a> </span></div></div></section><!--noindex--><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class=post-toc><div class=post-toc-content><ol class=nav><li class="nav-item nav-level-1"><a class=nav-link href=#%E5%89%8D%E8%A8%80><span class=nav-number>1.</span> <span class=nav-text>前言</span></a><li class="nav-item nav-level-1"><a class=nav-link href=#reference><span class=nav-number>2.</span> <span class=nav-text>Reference</span></a></ol></div></div></section><!--/noindex--></div></aside></div></main><footer class=footer id=footer><div class=footer-inner><div class=copyright>© <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-user"></i> </span><span class=author itemprop=copyrightHolder>FesianXu</span></div><div class=“theme-info”><div class=“powered-by”></div><span class=“post-count”> 该站点文章共457k字，欢迎光临~ </span></div><script async src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><div class=busuanzi-count><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span class=site-uv> <i class="fa fa-user"></i> <span class=busuanzi-value id=busuanzi_value_site_uv></span> </span><span class=site-pv> <i class="fa fa-eye"></i> <span class=busuanzi-value id=busuanzi_value_site_pv></span> </span></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i></div></div><script>if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }</script><script src=/lib/jquery/index.js></script><script src=/lib/fastclick/lib/fastclick.min.js></script><script src=/lib/jquery_lazyload/jquery.lazyload.js></script><script src=/lib/velocity/velocity.min.js></script><script src=/lib/velocity/velocity.ui.min.js></script><script src=/lib/fancybox/source/jquery.fancybox.pack.js></script><script src=/js/src/utils.js></script><script src=/js/src/motion.js></script><script src=/js/src/scrollspy.js></script><script src=/js/src/post-details.js></script><script src=/js/src/bootstrap.js></script><script>// Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.json";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });</script><script src=/live2dw/lib/L2Dwidget.min.js></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7}});</script>